{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - tensorflow\n",
    "# - keras\n",
    "# - python_speech_features\n",
    "# - imblearn \n",
    "# - Librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train set extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_extractor(file_name):\n",
    "\n",
    "    csv_open = open(file_name,\"r\")\n",
    "    feature_names = []\n",
    "    labels = []\n",
    "\n",
    "    for index, wave in enumerate(csv_open):\n",
    "        wave = wave.strip(\"\\n\")\n",
    "        wave = wave.split(\",\")\n",
    "    \n",
    "        feature = wave[0]\n",
    "        label = wave[1]\n",
    "        if index == 0:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            feature_names.append(feature)\n",
    "            labels.append(label)\n",
    "\n",
    "    feature_names = numpy.array(feature_names)\n",
    "    labels = numpy.array(labels)\n",
    "    \n",
    "    return feature_names, labels\n",
    "CSV_TRAIN = csv_extractor(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test set extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_extractor_test(file_name):\n",
    "\n",
    "    csv_open = open(file_name,\"r\")\n",
    "    \n",
    "    feature_name_test = []\n",
    "\n",
    "    for index, wave in enumerate(csv_open):\n",
    "        wave = wave.strip(\"\\n\")\n",
    "\n",
    "        feature = wave\n",
    "        if index == 0:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            print(index,feature)\n",
    "            feature_name_test.append(feature)\n",
    "\n",
    "    feature_name_test = numpy.array(feature_name_test)\n",
    "    \n",
    "    return feature_name_test\n",
    "\n",
    "CSV_TEST = csv_extractor_test(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store train and test in an array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names, labels = CSV_TRAIN \n",
    "feature_name_test = CSV_TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len of train(features) examples:\", feature_names.shape)\n",
    "print(\"len of test examples\",feature_name_test.shape)\n",
    "print()\n",
    "print(\"example of a file name:\", feature_names[-1])\n",
    "print(\"example of a label:\", labels[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 1: Open our Dataset & Explore the structure of wave samples\n",
    "- Okay, now that we have a list with all wave names and labels.\n",
    "<br>\n",
    "- Load wave file and visualize its waveform (using librosa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = 'C:/Users/voice/Downloads/dataset/wav/'\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data, sampling_rate = librosa.load(my_path+'00000b653e96c0697676f91ec9dfb9fc9b4d085f.wav')\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 2: match feature_names with wave_files and extract MFCC features\n",
    "\n",
    "- all_wave = folder with all wave files\n",
    "- feature_names(numpy array) = name of all wave files\n",
    "- labels(numpy array) = labels of all feature_names\n",
    "- function will return features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path_examples = []\n",
    "for example in os.listdir(my_path):\n",
    "    my_path_examples.append(example)\n",
    "    \n",
    "my_path_examples = numpy.array(my_path_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len of train(features) examples:\", feature_names.shape)\n",
    "print(\"len of train(labels) examples:\", labels.shape)\n",
    "print(\"len of train + test examples:\", my_path_examples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCC feature extraction (train set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_speech_features import mfcc\n",
    "from python_speech_features import logfbank\n",
    "import scipy.io.wavfile as wav\n",
    "import librosa\n",
    "\n",
    "def mfcc_features(my_path,feature_names, labels):\n",
    "\n",
    "    #mean_mfcc_list = []\n",
    "    #std_mfcc_list = []\n",
    "    features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for index, (wave_name,label) in enumerate(zip(feature_names,labels)):\n",
    "        \n",
    "        for wave in os.listdir(my_path): \n",
    "            #if wav and wave_name are equal\n",
    "            #extract mfcc features and append them to features\n",
    "            #append label to labels\n",
    "            if wave == wave_name:\n",
    "                #x,sr = librosa.load(my_path+wave)\n",
    "                #mean_mfcc = numpy.mean(librosa.feature.mfcc(x,sr,n_mfcc = 40).T, axis = 0)\n",
    "                #std_mfcc = numpy.std(librosa.feature.mfcc(x,sr,n_mfcc = 40).T, axis = 0)\n",
    "                (rate,sig) = wav.read(my_path+wave)\n",
    "                mfcc_feat = mfcc(sig,rate)\n",
    "                    \n",
    "                #mean_mfcc_list.append(mean_mfcc)\n",
    "                #std_mfcc_list.append(std_mfcc)\n",
    "                \n",
    "                all_labels.append(label)\n",
    "                print(index)\n",
    "                 \n",
    "    #mean_mfcc_list = numpy.array(mean_mfcc_list) \n",
    "    #std_mfcc_list = numpy.array(std_mfcc_list)\n",
    "    features = numpy.array(features)\n",
    "    all_labels = numpy.array(all_labels)\n",
    "    \n",
    "    return features, all_labels\n",
    "\n",
    "MFCC_FEATURES = mfcc_features(my_path, feature_names, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCC feature extraction (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_speech_features import mfcc\n",
    "from python_speech_features import logfbank\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "def mfcc_features_test(my_path,feature_names):\n",
    "\n",
    "    all_features = []\n",
    "    all_filename_test = []\n",
    "    \n",
    "    for index, wave_name in enumerate(feature_names):\n",
    "        \n",
    "        for wave in os.listdir(my_path): \n",
    "            #if wav and wave_name are equal\n",
    "            #extract mfcc features and append them to features\n",
    "            if wave == wave_name: \n",
    "                (rate,sig) = wav.read(my_path+wave)\n",
    "                mfcc_feat = mfcc(sig,rate, numcep=40)\n",
    "                all_filename_test.append(wave)\n",
    "                all_features.append(mfcc_feat)\n",
    "                print(index)\n",
    "                 \n",
    "    all_features = numpy.array(all_features) \n",
    "    all_filename_test = numpy.array(all_filename_test)\n",
    "    \n",
    "    return all_features, all_filename_test\n",
    "\n",
    "MFCC_FEATURES_test = mfcc_features_test(my_path, feature_name_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract features and labels for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MFCC_features, MFCC_labels = MFCC_FEATURES\n",
    "MFCC_features_test, all_labels_test = MFCC_FEATURES_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of extracted mfcc features from test set:\", MFCC_features_test.shape)\n",
    "print(\"shape of extracted mfcc features:\", MFCC_features[0].shape)\n",
    "print(\"shape of extracted mfcc labels:\", MFCC_labels.shape)\n",
    "print(\"shape of extracted filter bank energy from test set:\", LFBE_features_test.shape)\n",
    "print(\"shape of extracted log filter bank energy features:\", LFBE_features[0].shape)\n",
    "print(\"shape of extracted log filter bank energy labels:\", LFBE_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zero pad MFCC features train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_shape = 0\n",
    "for x in MFCC_features:\n",
    "    all_shapes = x.shape[0]\n",
    "    if all_shapes > max_shape:\n",
    "        max_shape = all_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_spoken_train = []\n",
    "for x in MFCC_features:\n",
    "    difference = max_shape-x.shape[0]\n",
    "    zero_padded = numpy.pad(x,((0,difference),(0,0)), \"constant\")\n",
    "    new_spoken_train.append(zero_padded)\n",
    "\n",
    "new_spoken_train = numpy.array(new_spoken_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zero pad MFCC features test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_shape_test = 0\n",
    "for x in MFCC_features_test:\n",
    "    all_shapes = x.shape[0]\n",
    "    if all_shapes > max_shape_test:\n",
    "        max_shape_test = all_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_spoken_test = []\n",
    "for x in MFCC_features_test:\n",
    "    difference = max_shape_test-x.shape[0]\n",
    "    zero_padded = numpy.pad(x,((0,difference),(0,0)), \"constant\")\n",
    "    new_spoken_test.append(zero_padded)\n",
    "\n",
    "new_spoken_test = numpy.array(new_spoken_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_spoken_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_spoken_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "#Fitting the PCA algorithm with our Data\n",
    "pca_train = PCA().fit(new_spoken_train)\n",
    "#Plotting the Cumulative Summation of the Explained Variance\n",
    "plt.figure()\n",
    "plt.plot(numpy.cumsum(pca_train.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('MFCC Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_spoken_train = numpy.reshape(new_spoken_train,(94824,1287))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(new_spoken_train)\n",
    "new_spoken_train = scaler.fit_transform(new_spoken_train)\n",
    "new_spoken_test = scaler.transform(new_spoken_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "ros = SMOTE(random_state=42)\n",
    "new_mfcc_features,  mfcc_labels = ros.fit_resample(new_spoken_train, MFCC_labels)\n",
    "\n",
    "dictionary_2 = {}\n",
    "for i in mfcc_labels:\n",
    "    dictionary_2[i] = dictionary_2.get(i,0) + 1\n",
    "\n",
    "print(dictionary_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reshape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mfcc_features = numpy.reshape(new_mfcc_features,(new_mfcc_features.shape[0],99,13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_spoken_test.shape)\n",
    "print(new_mfcc_features.shape)\n",
    "print(mfcc_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_spoken_test = numpy.reshape(new_spoken_test,(11005,1287))\n",
    "new_spoken_train = numpy.reshape(new_mfcc_features,(127190,1287))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# define example\n",
    "data = mfcc_labels\n",
    "values = array(data)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "#print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "new_mfcc_labels = onehot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import train_test_split and split data in train & val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_spoken_train, new_mfcc_labels, test_size=0.20, \n",
    "                                                    random_state=42, stratify = new_mfcc_labels, shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = numpy.reshape(X_train,(X_train.shape[0],99,13))\n",
    "X_test = numpy.reshape(X_test,(X_test.shape[0],99,13))\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\",y_train.shape)\n",
    "print(\"X_test shape:\",X_test.shape)\n",
    "print(\"y_test shape:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build classification model: 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from wandb import magic\n",
    "model = Sequential()\n",
    "model.add(Conv1D(128, 2, activation = \"relu\", input_shape=(99,13)))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Conv1D(256, 2, activation = \"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(356, 2, activation = \"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Conv1D(456, 2, activation = \"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=(2)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1048))\n",
    "model.add(Dense(35, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "model.compile(loss= \"categorical_crossentropy\",\n",
    "              optimizer= Adam(lr=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnnhistory= model.fit(X_train, y_train,batch_size= 64, epochs= 150 ,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## result after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print(\"validation accuracy:\", result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN prediction output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_spoken_test = numpy.reshape(new_spoken_test,(11005,99,13,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_open = open(\"result.csv\",\"a+\")\n",
    "for index in range(11005):\n",
    "    file_name = all_labels_test[index]\n",
    "    predicted_word = label_encoder.inverse_transform([argmax(model.predict(new_spoken_test)[index, :])])\n",
    "    predicted_word = \"'\".join(word)\n",
    "    file_open.write(file_name+\",\"+predicted_word+\"\\n\")\n",
    "    \n",
    "file_open.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
